{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Introduction - IMAGE TO SKETCH CONVERSION\n","Autoencoder is special type of deep learning architecture consisting of two networks a) encoder b)decoder. Encoder can be fully connected dense neural network or Convolution neural network. Encoder is used to downsample our original sample image into latent vector by passing image through Convolution layers and maxpool layer.Similary,decoder also can be fully connected neural network or Convolution neural network, decoder is used to upsample the latent vector downsampled by encoder. This upsampled latent vector is compared with the original input and reconstruction loss is calculated. Backpropagation is used to minimized this reconstruction loss. Simple autoencoder can be used for Domain transformation, denoising images, image colorization, anamoly detection etc. Here I am going to train my autoencoder model to generate sketch of the input image.  I don't have enough training images so my model mightnot generate very good sketch of image."]},{"cell_type":"markdown","metadata":{},"source":["<img src = 'https://www.researchgate.net/profile/Chitralekha_Bhat/publication/317559243/figure/fig2/AS:531269123805186@1503675837486/Deep-Autoencoder-DAE.png'>"]},{"cell_type":"markdown","metadata":{},"source":["## Objective:\n","To convert image to sketch using autoencoder"]},{"cell_type":"markdown","metadata":{},"source":["## Import Necessary Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["\n","import numpy as np\n","import tensorflow as tf\n","import keras \n","from keras.layers import Dense, Conv2D, MaxPool2D, UpSampling2D, Dropout, Input\n","from keras.preprocessing.image import img_to_array\n","import matplotlib.pyplot as plt\n","import cv2\n","from tqdm import tqdm \n","import os\n","import re"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Load data\n","This dataset consist of 188 image and their corresponding sketches. As these images aren't enough for training our autoencoder model, we have augmented them using open cv library. After Augmentation we have got around 1500 images, these 1500 images. These images are converted into array and are stored in the list."]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"outputs":[],"source":["# to get the files in proper order\n","def sorted_alphanumeric(data):  \n","    convert = lambda text: int(text) if text.isdigit() else text.lower()\n","    alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)',key)]\n","    return sorted(data,key = alphanum_key)\n","\n","\n","# defining the size of image \n","SIZE = 256\n","\n","image_path = '../input/cuhk-face-sketch-database-cufs/photos'\n","img_array = []\n","\n","sketch_path = '../input/cuhk-face-sketch-database-cufs/sketches'\n","sketch_array = []\n","\n","image_file = sorted_alphanumeric(os.listdir(image_path))\n","sketch_file = sorted_alphanumeric(os.listdir(sketch_path))\n","\n","\n","for i in tqdm(image_file):\n","    image = cv2.imread(image_path + '/' + i,1)\n","    \n","    # as opencv load image in bgr format converting it to rgb\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    \n","    # resizing images \n","    image = cv2.resize(image, (SIZE, SIZE))\n","    \n","    # normalizing image \n","    image = image.astype('float32') / 255.0\n","    \n","    #appending normal normal image    \n","    img_array.append(img_to_array(image))\n","    # Image Augmentation\n","    \n","    # horizontal flip \n","    img1 = cv2.flip(image,1)\n","    img_array.append(img_to_array(img1))\n","     #vertical flip \n","    img2 = cv2.flip(image,-1)\n","    img_array.append(img_to_array(img2))\n","     #vertical flip \n","    img3 = cv2.flip(image,-1)\n","    # horizontal flip\n","    img3 = cv2.flip(img3,1)\n","    img_array.append(img_to_array(img3))\n","    # rotate clockwise \n","    img4 = cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE)\n","    img_array.append(img_to_array(img4))\n","    # flip rotated image \n","    img5 = cv2.flip(img4,1)\n","    img_array.append(img_to_array(img5))\n","     # rotate anti clockwise \n","    img6 = cv2.rotate(image, cv2.ROTATE_90_COUNTERCLOCKWISE)\n","    img_array.append(img_to_array(img6))\n","    # flip rotated image \n","    img7 = cv2.flip(img6,1)\n","    img_array.append(img_to_array(img7))\n","  \n","    \n","for i in tqdm(sketch_file):\n","    image = cv2.imread(sketch_path + '/' + i,1)\n","    \n","    # as opencv load image in bgr format converting it to rgb\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","    \n","    # resizing images \n","    image = cv2.resize(image, (SIZE, SIZE))\n","    \n","    # normalizing image \n","    image = image.astype('float32') / 255.0\n","    # appending normal sketch image\n","    sketch_array.append(img_to_array(image))\n","    \n","    #Image Augmentation\n","    # horizontal flip \n","    img1 = cv2.flip(image,1)\n","    sketch_array.append(img_to_array(img1))\n","     #vertical flip \n","    img2 = cv2.flip(image,-1)\n","    sketch_array.append(img_to_array(img2))\n","     #vertical flip \n","    img3 = cv2.flip(image,-1)\n","    # horizontal flip\n","    img3 = cv2.flip(img3,1)\n","    sketch_array.append(img_to_array(img3))\n","    # rotate clockwise \n","    img4 = cv2.rotate(image, cv2.ROTATE_90_CLOCKWISE)\n","    sketch_array.append(img_to_array(img4))\n","    # flip rotated image \n","    img5 = cv2.flip(img4,1)\n","    sketch_array.append(img_to_array(img5))\n","     # rotate anti clockwise \n","    img6 = cv2.rotate(image, cv2.ROTATE_90_COUNTERCLOCKWISE)\n","    sketch_array.append(img_to_array(img6))\n","    # flip rotated image \n","    img7 = cv2.flip(img6,1)\n","    sketch_array.append(img_to_array(img7))\n","    \n","    \n","    \n","   \n","\n","\n","\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(\"Total number of sketch images:\",len(sketch_array))\n","print(\"Total number of images:\",len(img_array))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Visualizing images\n","Here we have plotted all augmented images and its augmented sketches"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# defining function to plot images pair\n","def plot_images(image, sketches):\n","    plt.figure(figsize=(7,7))\n","    plt.subplot(1,2,1)\n","    plt.title('Image', color = 'green', fontsize = 20)\n","    plt.imshow(image)\n","    plt.subplot(1,2,2)\n","    plt.title('Sketches ', color = 'black', fontsize = 20)\n","    plt.imshow(sketches)\n","   \n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["ls = [i for i in range(0,65,8)]\n","for i in ls:\n","    plot_images(img_array[i],sketch_array[i])\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\n","## Slicing and reshaping\n","\n","Out of 1504 images We have sliced them to two part. train images consist 1400 images while test images contains 104 images. After slicing image array, we reshaped them so that images can be fed directly into our encoder network\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_sketch_image = sketch_array[:1400]\n","train_image = img_array[:1400]\n","test_sketch_image = sketch_array[1400:]\n","test_image = img_array[1400:]\n","# reshaping\n","train_sketch_image = np.reshape(train_sketch_image,(len(train_sketch_image),SIZE,SIZE,3))\n","train_image = np.reshape(train_image, (len(train_image),SIZE,SIZE,3))\n","print('Train color image shape:',train_image.shape)\n","test_sketch_image = np.reshape(test_sketch_image,(len(test_sketch_image),SIZE,SIZE,3))\n","test_image = np.reshape(test_image, (len(test_image),SIZE,SIZE,3))\n","print('Test color image shape',test_image.shape)"]},{"cell_type":"markdown","metadata":{},"source":["## Downsample layer"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def downsample(filters, size, apply_batch_normalization = True):\n","    downsample = tf.keras.models.Sequential()\n","    downsample.add(keras.layers.Conv2D(filters = filters, kernel_size = size, strides = 2, use_bias = False, kernel_initializer = 'he_normal'))\n","    if apply_batch_normalization:\n","        downsample.add(keras.layers.BatchNormalization())\n","    downsample.add(keras.layers.LeakyReLU())\n","    return downsample"]},{"cell_type":"markdown","metadata":{},"source":["## Upsample Layer"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def upsample(filters, size, apply_dropout = False):\n","    upsample = tf.keras.models.Sequential()\n","    upsample.add(keras.layers.Conv2DTranspose(filters = filters, kernel_size = size, strides = 2, use_bias = False, kernel_initializer = 'he_normal'))\n","    if apply_dropout:\n","        upsample.add(tf.keras.layers.Dropout(0.1))\n","    upsample.add(tf.keras.layers.LeakyReLU()) \n","    return upsample"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Model \n","Here we have use sequence of downsample layer for encoder and upsample layer for decoder"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def model():\n","    encoder_input = keras.Input(shape = (SIZE, SIZE, 3))\n","    x = downsample(16, 4, False)(encoder_input)\n","    x = downsample(32,4)(x)\n","    x = downsample(64,4,False)(x)\n","    x = downsample(128,4)(x)\n","    x = downsample(256,4)(x)\n","   \n","    encoder_output = downsample(512,4)(x)\n","    \n","    decoder_input = upsample(512,4,True)(encoder_output)\n","    x = upsample(256,4,False)(decoder_input)\n","    x = upsample(128,4, True)(x)\n","    x = upsample(64,4)(x)\n","    x = upsample(32,4)(x)\n","    x = upsample(16,4)(x)\n","    x = tf.keras.layers.Conv2DTranspose(8,(2,2),strides = (1,1), padding = 'valid')(x)\n","    decoder_output = tf.keras.layers.Conv2DTranspose(3,(2,2),strides = (1,1), padding = 'valid')(x)\n","    \n","  \n","    return tf.keras.Model(encoder_input, decoder_output)\n","\n","        \n","    \n","    \n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# to get summary of model\n","model = model()\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# ## Model\n","# Here we have defined two blocks of networks. Encoder network takes 256 by 256 image and downsample it to 16 by 16 latent vector\n","# by passing our image via series of Convolution and Maxpooling layer. This downsampled 16 by 16 latent vector is upsampled by passing \n","# through series of Convolution and UpSampling layer. The final decoder output is same as our encoder input. This upsamples output of decoder\n","# is compared with our sketches and reconstruction loss is calculated. This loss is minimized by updating weight and bias of network through\n","# backpropagation.\n","# encoder_input = keras.Input(shape=(SIZE,SIZE, 3), name=\"img\")\n","# x = Conv2D(filters = 16, kernel_size = (3,3), activation = 'relu', padding = 'same')(encoder_input)\n","# x = MaxPool2D(pool_size = (2,2))(x)\n","\n","# x = Conv2D(filters = 32,kernel_size = (3,3),strides = (2,2), activation = 'relu', padding = 'valid')(x)\n","# x = Conv2D(filters = 64, kernel_size = (3,3), strides = (2,2), activation = 'relu', padding = 'same')(x)\n","# x = MaxPool2D(pool_size = (2,2))(x)\n","\n","# x = Conv2D(filters = 128, kernel_size = (3,3), activation = 'relu', padding = 'same')(x)\n","# x = Conv2D(filters = 256 , kernel_size = (3,3), activation = 'relu', padding = 'same')(x) \n","# encoder_output = Conv2D(filters = 512 , kernel_size = (3,3), activation = 'relu', padding = 'same')(x) \n","# encoder = tf.keras.Model(encoder_input, encoder_output)\n","\n","# decoder_input = Conv2D(filters = 512 ,kernel_size = (3,3), activation = 'relu', padding = 'same')(encoder_output)\n","# x = UpSampling2D(size = (2,2))(decoder_input)\n","# x = Conv2D(filters = 256, kernel_size = (3,3), activation = 'relu', padding = 'same')(x)\n","# x = Conv2D(filters = 128, kernel_size = (3,3), activation = 'relu', padding = 'same')(x)\n","# x = UpSampling2D(size = (2,2) )(x)\n","\n","# x = Conv2D(filters = 64, kernel_size = (3,3), activation = 'relu', padding = 'same')(x)\n","# x = UpSampling2D(size = (2,2) )(x)\n","# x = Conv2D(filters = 32 , kernel_size = (3,3), activation = 'relu', padding = 'same')(x)\n","# x = UpSampling2D(size = (2,2) )(x)\n"," \n","# x = Conv2D(filters = 16  , kernel_size = (3,3), activation = 'relu', padding = 'same')(x)\n","# decoder_output = Conv2D(filters = 3, kernel_size = (3,3), activation = 'relu', padding = 'same')(x)\n","\n","# # final model\n","# model = keras.Model(encoder_input, decoder_output)\n","# model.summary()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Compiling and Fitting our model\n","Here we have used Adam optimizer and mean_squared_error as loss and have trained model for 100 epochs"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001), loss = 'mean_absolute_error',\n","              metrics = ['acc'])\n","\n","model.fit(train_image, train_sketch_image, epochs = 100, verbose = 0)"]},{"cell_type":"markdown","metadata":{},"source":["### Evaluating our model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["prediction_on_test_data = model.evaluate(test_image, test_sketch_image)\n","print(\"Loss: \", prediction_on_test_data[0])\n","print(\"Accuracy: \", np.round(prediction_on_test_data[1] * 100,1))"]},{"cell_type":"markdown","metadata":{},"source":["# Plotting our predicted sketch along with real sketch"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def show_images(real,sketch, predicted):\n","    plt.figure(figsize = (12,12))\n","    plt.subplot(1,3,1)\n","    plt.title(\"Image\",fontsize = 15, color = 'Lime')\n","    plt.imshow(real)\n","    plt.subplot(1,3,2)\n","    plt.title(\"sketch\",fontsize = 15, color = 'Blue')\n","    plt.imshow(sketch)\n","    plt.subplot(1,3,3)\n","    plt.title(\"Predicted\",fontsize = 15, color = 'gold')\n","    plt.imshow(predicted)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["ls = [i for i in range(0,95,8)]\n","for i in ls:\n","    predicted =np.clip(model.predict(test_image[i].reshape(1,SIZE,SIZE,3)),0.0,1.0).reshape(SIZE,SIZE,3)\n","    show_images(test_image[i],test_sketch_image[i],predicted)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":4}
